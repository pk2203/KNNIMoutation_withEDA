{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-07-02T18:43:13.228649Z","iopub.execute_input":"2022-07-02T18:43:13.229059Z","iopub.status.idle":"2022-07-02T18:43:13.261861Z","shell.execute_reply.started":"2022-07-02T18:43:13.228970Z","shell.execute_reply":"2022-07-02T18:43:13.260424Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/tabular-playground-series-jun-2022/sample_submission.csv\n/kaggle/input/tabular-playground-series-jun-2022/data.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch \nfrom tqdm.notebook import tqdm as tqdm\nimport torch.nn.functional as func ","metadata":{"execution":{"iopub.status.busy":"2022-07-02T18:43:15.371761Z","iopub.execute_input":"2022-07-02T18:43:15.372534Z","iopub.status.idle":"2022-07-02T18:43:16.989232Z","shell.execute_reply.started":"2022-07-02T18:43:15.372495Z","shell.execute_reply":"2022-07-02T18:43:16.988236Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('../input/tabular-playground-series-jun-2022/data.csv')\nres = pd.read_csv('../input/tabular-playground-series-jun-2022/sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2022-07-02T18:43:19.879322Z","iopub.execute_input":"2022-07-02T18:43:19.880826Z","iopub.status.idle":"2022-07-02T18:43:36.503695Z","shell.execute_reply.started":"2022-07-02T18:43:19.880772Z","shell.execute_reply":"2022-07-02T18:43:36.502707Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"df.drop('row_id',axis = 1,inplace = True)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-07-02T18:43:41.775494Z","iopub.execute_input":"2022-07-02T18:43:41.776598Z","iopub.status.idle":"2022-07-02T18:43:41.913862Z","shell.execute_reply.started":"2022-07-02T18:43:41.776570Z","shell.execute_reply":"2022-07-02T18:43:41.912204Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"      F_1_0     F_1_1     F_1_2     F_1_3     F_1_4     F_1_5     F_1_6  \\\n0 -0.354591 -0.464038  2.304115  0.734486  1.696395  0.136285 -0.518344   \n1  1.380940 -0.499626 -0.418548  1.911725 -0.826130 -1.715371 -0.577091   \n2  0.256023 -1.059874       NaN  0.345678  1.513814  1.243864 -0.509648   \n3 -0.728420 -2.432399 -2.453602 -0.020509  0.333397  0.086049 -1.787601   \n4  0.590212 -0.066127  0.468009 -1.096038  0.119399 -1.809710  0.466358   \n\n      F_1_7     F_1_8     F_1_9  ...     F_4_5     F_4_6     F_4_7     F_4_8  \\\n0  0.502640 -1.852504 -0.500665  ...  3.744152  0.794438  0.265185 -0.561809   \n1 -1.041486  0.596067 -0.363425  ... -2.895826 -0.738275  2.361818 -0.060753   \n2 -0.800481 -0.115945  0.595777  ...  2.252834  0.472496  2.491386  0.353381   \n3  0.667011  0.761564 -2.217847  ...  2.004600 -4.664806 -0.847211 -0.264249   \n4 -0.053196 -0.580320 -1.143500  ...  0.976937  2.558883  3.377724  0.846891   \n\n      F_4_9    F_4_10    F_4_11    F_4_12    F_4_13    F_4_14  \n0  0.196480  0.373434  6.206995  3.809505  1.236486  1.182055  \n1  0.727249 -0.271882  5.232157 -4.218259 -2.724883 -0.063775  \n2 -0.260682 -0.000833 -0.116457 -2.131747  3.661499 -0.131576  \n3  0.664334 -0.557868  8.499483 -4.738799 -3.054611  0.494152  \n4  0.696032  0.554121 -5.979714 -2.869631  3.733057 -0.722943  \n\n[5 rows x 80 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>F_1_0</th>\n      <th>F_1_1</th>\n      <th>F_1_2</th>\n      <th>F_1_3</th>\n      <th>F_1_4</th>\n      <th>F_1_5</th>\n      <th>F_1_6</th>\n      <th>F_1_7</th>\n      <th>F_1_8</th>\n      <th>F_1_9</th>\n      <th>...</th>\n      <th>F_4_5</th>\n      <th>F_4_6</th>\n      <th>F_4_7</th>\n      <th>F_4_8</th>\n      <th>F_4_9</th>\n      <th>F_4_10</th>\n      <th>F_4_11</th>\n      <th>F_4_12</th>\n      <th>F_4_13</th>\n      <th>F_4_14</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-0.354591</td>\n      <td>-0.464038</td>\n      <td>2.304115</td>\n      <td>0.734486</td>\n      <td>1.696395</td>\n      <td>0.136285</td>\n      <td>-0.518344</td>\n      <td>0.502640</td>\n      <td>-1.852504</td>\n      <td>-0.500665</td>\n      <td>...</td>\n      <td>3.744152</td>\n      <td>0.794438</td>\n      <td>0.265185</td>\n      <td>-0.561809</td>\n      <td>0.196480</td>\n      <td>0.373434</td>\n      <td>6.206995</td>\n      <td>3.809505</td>\n      <td>1.236486</td>\n      <td>1.182055</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.380940</td>\n      <td>-0.499626</td>\n      <td>-0.418548</td>\n      <td>1.911725</td>\n      <td>-0.826130</td>\n      <td>-1.715371</td>\n      <td>-0.577091</td>\n      <td>-1.041486</td>\n      <td>0.596067</td>\n      <td>-0.363425</td>\n      <td>...</td>\n      <td>-2.895826</td>\n      <td>-0.738275</td>\n      <td>2.361818</td>\n      <td>-0.060753</td>\n      <td>0.727249</td>\n      <td>-0.271882</td>\n      <td>5.232157</td>\n      <td>-4.218259</td>\n      <td>-2.724883</td>\n      <td>-0.063775</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.256023</td>\n      <td>-1.059874</td>\n      <td>NaN</td>\n      <td>0.345678</td>\n      <td>1.513814</td>\n      <td>1.243864</td>\n      <td>-0.509648</td>\n      <td>-0.800481</td>\n      <td>-0.115945</td>\n      <td>0.595777</td>\n      <td>...</td>\n      <td>2.252834</td>\n      <td>0.472496</td>\n      <td>2.491386</td>\n      <td>0.353381</td>\n      <td>-0.260682</td>\n      <td>-0.000833</td>\n      <td>-0.116457</td>\n      <td>-2.131747</td>\n      <td>3.661499</td>\n      <td>-0.131576</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-0.728420</td>\n      <td>-2.432399</td>\n      <td>-2.453602</td>\n      <td>-0.020509</td>\n      <td>0.333397</td>\n      <td>0.086049</td>\n      <td>-1.787601</td>\n      <td>0.667011</td>\n      <td>0.761564</td>\n      <td>-2.217847</td>\n      <td>...</td>\n      <td>2.004600</td>\n      <td>-4.664806</td>\n      <td>-0.847211</td>\n      <td>-0.264249</td>\n      <td>0.664334</td>\n      <td>-0.557868</td>\n      <td>8.499483</td>\n      <td>-4.738799</td>\n      <td>-3.054611</td>\n      <td>0.494152</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.590212</td>\n      <td>-0.066127</td>\n      <td>0.468009</td>\n      <td>-1.096038</td>\n      <td>0.119399</td>\n      <td>-1.809710</td>\n      <td>0.466358</td>\n      <td>-0.053196</td>\n      <td>-0.580320</td>\n      <td>-1.143500</td>\n      <td>...</td>\n      <td>0.976937</td>\n      <td>2.558883</td>\n      <td>3.377724</td>\n      <td>0.846891</td>\n      <td>0.696032</td>\n      <td>0.554121</td>\n      <td>-5.979714</td>\n      <td>-2.869631</td>\n      <td>3.733057</td>\n      <td>-0.722943</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 80 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df.fillna(0, inplace = True)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-07-02T18:43:44.581204Z","iopub.execute_input":"2022-07-02T18:43:44.581587Z","iopub.status.idle":"2022-07-02T18:43:44.679716Z","shell.execute_reply.started":"2022-07-02T18:43:44.581557Z","shell.execute_reply":"2022-07-02T18:43:44.678743Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"      F_1_0     F_1_1     F_1_2     F_1_3     F_1_4     F_1_5     F_1_6  \\\n0 -0.354591 -0.464038  2.304115  0.734486  1.696395  0.136285 -0.518344   \n1  1.380940 -0.499626 -0.418548  1.911725 -0.826130 -1.715371 -0.577091   \n2  0.256023 -1.059874  0.000000  0.345678  1.513814  1.243864 -0.509648   \n3 -0.728420 -2.432399 -2.453602 -0.020509  0.333397  0.086049 -1.787601   \n4  0.590212 -0.066127  0.468009 -1.096038  0.119399 -1.809710  0.466358   \n\n      F_1_7     F_1_8     F_1_9  ...     F_4_5     F_4_6     F_4_7     F_4_8  \\\n0  0.502640 -1.852504 -0.500665  ...  3.744152  0.794438  0.265185 -0.561809   \n1 -1.041486  0.596067 -0.363425  ... -2.895826 -0.738275  2.361818 -0.060753   \n2 -0.800481 -0.115945  0.595777  ...  2.252834  0.472496  2.491386  0.353381   \n3  0.667011  0.761564 -2.217847  ...  2.004600 -4.664806 -0.847211 -0.264249   \n4 -0.053196 -0.580320 -1.143500  ...  0.976937  2.558883  3.377724  0.846891   \n\n      F_4_9    F_4_10    F_4_11    F_4_12    F_4_13    F_4_14  \n0  0.196480  0.373434  6.206995  3.809505  1.236486  1.182055  \n1  0.727249 -0.271882  5.232157 -4.218259 -2.724883 -0.063775  \n2 -0.260682 -0.000833 -0.116457 -2.131747  3.661499 -0.131576  \n3  0.664334 -0.557868  8.499483 -4.738799 -3.054611  0.494152  \n4  0.696032  0.554121 -5.979714 -2.869631  3.733057 -0.722943  \n\n[5 rows x 80 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>F_1_0</th>\n      <th>F_1_1</th>\n      <th>F_1_2</th>\n      <th>F_1_3</th>\n      <th>F_1_4</th>\n      <th>F_1_5</th>\n      <th>F_1_6</th>\n      <th>F_1_7</th>\n      <th>F_1_8</th>\n      <th>F_1_9</th>\n      <th>...</th>\n      <th>F_4_5</th>\n      <th>F_4_6</th>\n      <th>F_4_7</th>\n      <th>F_4_8</th>\n      <th>F_4_9</th>\n      <th>F_4_10</th>\n      <th>F_4_11</th>\n      <th>F_4_12</th>\n      <th>F_4_13</th>\n      <th>F_4_14</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-0.354591</td>\n      <td>-0.464038</td>\n      <td>2.304115</td>\n      <td>0.734486</td>\n      <td>1.696395</td>\n      <td>0.136285</td>\n      <td>-0.518344</td>\n      <td>0.502640</td>\n      <td>-1.852504</td>\n      <td>-0.500665</td>\n      <td>...</td>\n      <td>3.744152</td>\n      <td>0.794438</td>\n      <td>0.265185</td>\n      <td>-0.561809</td>\n      <td>0.196480</td>\n      <td>0.373434</td>\n      <td>6.206995</td>\n      <td>3.809505</td>\n      <td>1.236486</td>\n      <td>1.182055</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.380940</td>\n      <td>-0.499626</td>\n      <td>-0.418548</td>\n      <td>1.911725</td>\n      <td>-0.826130</td>\n      <td>-1.715371</td>\n      <td>-0.577091</td>\n      <td>-1.041486</td>\n      <td>0.596067</td>\n      <td>-0.363425</td>\n      <td>...</td>\n      <td>-2.895826</td>\n      <td>-0.738275</td>\n      <td>2.361818</td>\n      <td>-0.060753</td>\n      <td>0.727249</td>\n      <td>-0.271882</td>\n      <td>5.232157</td>\n      <td>-4.218259</td>\n      <td>-2.724883</td>\n      <td>-0.063775</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.256023</td>\n      <td>-1.059874</td>\n      <td>0.000000</td>\n      <td>0.345678</td>\n      <td>1.513814</td>\n      <td>1.243864</td>\n      <td>-0.509648</td>\n      <td>-0.800481</td>\n      <td>-0.115945</td>\n      <td>0.595777</td>\n      <td>...</td>\n      <td>2.252834</td>\n      <td>0.472496</td>\n      <td>2.491386</td>\n      <td>0.353381</td>\n      <td>-0.260682</td>\n      <td>-0.000833</td>\n      <td>-0.116457</td>\n      <td>-2.131747</td>\n      <td>3.661499</td>\n      <td>-0.131576</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-0.728420</td>\n      <td>-2.432399</td>\n      <td>-2.453602</td>\n      <td>-0.020509</td>\n      <td>0.333397</td>\n      <td>0.086049</td>\n      <td>-1.787601</td>\n      <td>0.667011</td>\n      <td>0.761564</td>\n      <td>-2.217847</td>\n      <td>...</td>\n      <td>2.004600</td>\n      <td>-4.664806</td>\n      <td>-0.847211</td>\n      <td>-0.264249</td>\n      <td>0.664334</td>\n      <td>-0.557868</td>\n      <td>8.499483</td>\n      <td>-4.738799</td>\n      <td>-3.054611</td>\n      <td>0.494152</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.590212</td>\n      <td>-0.066127</td>\n      <td>0.468009</td>\n      <td>-1.096038</td>\n      <td>0.119399</td>\n      <td>-1.809710</td>\n      <td>0.466358</td>\n      <td>-0.053196</td>\n      <td>-0.580320</td>\n      <td>-1.143500</td>\n      <td>...</td>\n      <td>0.976937</td>\n      <td>2.558883</td>\n      <td>3.377724</td>\n      <td>0.846891</td>\n      <td>0.696032</td>\n      <td>0.554121</td>\n      <td>-5.979714</td>\n      <td>-2.869631</td>\n      <td>3.733057</td>\n      <td>-0.722943</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 80 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2022-07-02T18:43:47.336244Z","iopub.execute_input":"2022-07-02T18:43:47.337003Z","iopub.status.idle":"2022-07-02T18:43:47.457752Z","shell.execute_reply.started":"2022-07-02T18:43:47.336938Z","shell.execute_reply":"2022-07-02T18:43:47.457106Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 1000000 entries, 0 to 999999\nData columns (total 80 columns):\n #   Column  Non-Null Count    Dtype  \n---  ------  --------------    -----  \n 0   F_1_0   1000000 non-null  float64\n 1   F_1_1   1000000 non-null  float64\n 2   F_1_2   1000000 non-null  float64\n 3   F_1_3   1000000 non-null  float64\n 4   F_1_4   1000000 non-null  float64\n 5   F_1_5   1000000 non-null  float64\n 6   F_1_6   1000000 non-null  float64\n 7   F_1_7   1000000 non-null  float64\n 8   F_1_8   1000000 non-null  float64\n 9   F_1_9   1000000 non-null  float64\n 10  F_1_10  1000000 non-null  float64\n 11  F_1_11  1000000 non-null  float64\n 12  F_1_12  1000000 non-null  float64\n 13  F_1_13  1000000 non-null  float64\n 14  F_1_14  1000000 non-null  float64\n 15  F_2_0   1000000 non-null  int64  \n 16  F_2_1   1000000 non-null  int64  \n 17  F_2_2   1000000 non-null  int64  \n 18  F_2_3   1000000 non-null  int64  \n 19  F_2_4   1000000 non-null  int64  \n 20  F_2_5   1000000 non-null  int64  \n 21  F_2_6   1000000 non-null  int64  \n 22  F_2_7   1000000 non-null  int64  \n 23  F_2_8   1000000 non-null  int64  \n 24  F_2_9   1000000 non-null  int64  \n 25  F_2_10  1000000 non-null  int64  \n 26  F_2_11  1000000 non-null  int64  \n 27  F_2_12  1000000 non-null  int64  \n 28  F_2_13  1000000 non-null  int64  \n 29  F_2_14  1000000 non-null  int64  \n 30  F_2_15  1000000 non-null  int64  \n 31  F_2_16  1000000 non-null  int64  \n 32  F_2_17  1000000 non-null  int64  \n 33  F_2_18  1000000 non-null  int64  \n 34  F_2_19  1000000 non-null  int64  \n 35  F_2_20  1000000 non-null  int64  \n 36  F_2_21  1000000 non-null  int64  \n 37  F_2_22  1000000 non-null  int64  \n 38  F_2_23  1000000 non-null  int64  \n 39  F_2_24  1000000 non-null  int64  \n 40  F_3_0   1000000 non-null  float64\n 41  F_3_1   1000000 non-null  float64\n 42  F_3_2   1000000 non-null  float64\n 43  F_3_3   1000000 non-null  float64\n 44  F_3_4   1000000 non-null  float64\n 45  F_3_5   1000000 non-null  float64\n 46  F_3_6   1000000 non-null  float64\n 47  F_3_7   1000000 non-null  float64\n 48  F_3_8   1000000 non-null  float64\n 49  F_3_9   1000000 non-null  float64\n 50  F_3_10  1000000 non-null  float64\n 51  F_3_11  1000000 non-null  float64\n 52  F_3_12  1000000 non-null  float64\n 53  F_3_13  1000000 non-null  float64\n 54  F_3_14  1000000 non-null  float64\n 55  F_3_15  1000000 non-null  float64\n 56  F_3_16  1000000 non-null  float64\n 57  F_3_17  1000000 non-null  float64\n 58  F_3_18  1000000 non-null  float64\n 59  F_3_19  1000000 non-null  float64\n 60  F_3_20  1000000 non-null  float64\n 61  F_3_21  1000000 non-null  float64\n 62  F_3_22  1000000 non-null  float64\n 63  F_3_23  1000000 non-null  float64\n 64  F_3_24  1000000 non-null  float64\n 65  F_4_0   1000000 non-null  float64\n 66  F_4_1   1000000 non-null  float64\n 67  F_4_2   1000000 non-null  float64\n 68  F_4_3   1000000 non-null  float64\n 69  F_4_4   1000000 non-null  float64\n 70  F_4_5   1000000 non-null  float64\n 71  F_4_6   1000000 non-null  float64\n 72  F_4_7   1000000 non-null  float64\n 73  F_4_8   1000000 non-null  float64\n 74  F_4_9   1000000 non-null  float64\n 75  F_4_10  1000000 non-null  float64\n 76  F_4_11  1000000 non-null  float64\n 77  F_4_12  1000000 non-null  float64\n 78  F_4_13  1000000 non-null  float64\n 79  F_4_14  1000000 non-null  float64\ndtypes: float64(55), int64(25)\nmemory usage: 610.4 MB\n","output_type":"stream"}]},{"cell_type":"code","source":"data = df.to_numpy()\ndata","metadata":{"execution":{"iopub.status.busy":"2022-07-02T18:43:52.053758Z","iopub.execute_input":"2022-07-02T18:43:52.054104Z","iopub.status.idle":"2022-07-02T18:43:52.155538Z","shell.execute_reply.started":"2022-07-02T18:43:52.054076Z","shell.execute_reply":"2022-07-02T18:43:52.154338Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"array([[-3.545910e-01, -4.640380e-01,  2.304115e+00, ...,  3.809505e+00,\n         1.236486e+00,  1.182055e+00],\n       [ 1.380940e+00, -4.996260e-01, -4.185480e-01, ..., -4.218259e+00,\n        -2.724883e+00, -6.377500e-02],\n       [ 2.560230e-01, -1.059874e+00,  0.000000e+00, ..., -2.131747e+00,\n         3.661499e+00, -1.315760e-01],\n       ...,\n       [ 1.475340e-01, -7.152760e-01, -4.650490e-01, ..., -5.841080e-01,\n        -1.492096e+00, -9.975020e-01],\n       [-1.709886e+00, -8.137850e-01, -1.866536e+00, ..., -1.085554e+00,\n         3.122423e+00,  4.831000e-03],\n       [-8.063800e-01, -2.525100e-02, -8.754770e-01, ...,  2.334131e+00,\n         5.425421e+00, -8.288470e-01]])"},"metadata":{}}]},{"cell_type":"code","source":"# System Parameters;\nmin_batch = 128\nmiss_rate = 0.018\ntrain_rate = 0.8\nalpha = 10\n# 3. Hint rate\np_hint = 0.9\n\nnum = data.shape[0]\ndim = data.shape[1]\n\nh_dim1 = dim\nh_dim2 = dim\n\nmin_val = np.zeros(dim)\nmax_val = np.zeros(dim)\nrng = np.random.RandomState()\n#Normalization\nfor i in range(dim):\n    min_val[i] = np.min(data[:,i])\n    data[:,i] = data[:,i] - np.min(data[:,i])\n    max_val[i] = np.max(data[:,i])\n    data[:,i] = data[:,i]/(np.max(data[:,i])+ 1e-6)\n#Missing value matrix   \nmat_miss_ind = int(num*miss_rate)\nmissing_mat = np.ones(data.shape)\nfor i in range(dim):\n    A = missing_mat[:,i]\n    A[:mat_miss_ind] = 0.\n    rng.shuffle(A)\n    missing_mat[:,i] = A\n\nrand_index = np.random.permutation(num)\ntrain_ind = int(num * train_rate)\n\ntrainX = data[rand_index[:train_ind], :]\ntestX = data[rand_index[train_ind:], :]\n\nMisstrainX = missing_mat[rand_index[:train_ind], :]\nMisstestX = missing_mat[rand_index[train_ind:], :]","metadata":{"execution":{"iopub.status.busy":"2022-07-02T19:00:25.036898Z","iopub.execute_input":"2022-07-02T19:00:25.037309Z","iopub.status.idle":"2022-07-02T19:00:37.627644Z","shell.execute_reply.started":"2022-07-02T19:00:25.037279Z","shell.execute_reply":"2022-07-02T19:00:37.626515Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"def sample_X (new_x, size):\n    ind = np.random.permutation(new_x)\n    return ind[:size]","metadata":{"execution":{"iopub.status.busy":"2022-07-02T18:44:17.860645Z","iopub.execute_input":"2022-07-02T18:44:17.860995Z","iopub.status.idle":"2022-07-02T18:44:17.866266Z","shell.execute_reply.started":"2022-07-02T18:44:17.860966Z","shell.execute_reply":"2022-07-02T18:44:17.865404Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def sample_Z (m,n):\n    return np.random.uniform(0.,0.01,size=[m,n])\n\ndef sample_M (u,v,rate):\n    A = np.ones((u,v))\n    miss_ind = int(u*rate)\n    for i in range(v):\n        B = A[:,i]\n        B[:miss_ind] = 0.\n        rng.shuffle(B)\n        A[:,i] = B\n    return A\n        \ndef xavier_init(size):\n    stddev = 1/np.sqrt(size[0]/2.)\n    return np.random.normal(size=size,scale =stddev)\n","metadata":{"execution":{"iopub.status.busy":"2022-07-02T19:01:04.949327Z","iopub.execute_input":"2022-07-02T19:01:04.949743Z","iopub.status.idle":"2022-07-02T19:01:04.958663Z","shell.execute_reply.started":"2022-07-02T19:01:04.949713Z","shell.execute_reply":"2022-07-02T19:01:04.957887Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"#Generator architecture\nG_w1 = torch.tensor(xavier_init([dim*2,h_dim1]),requires_grad = True)     # Data + Mask\nG_b1 = torch.tensor(np.zeros(h_dim1), requires_grad = True)\n\nG_w2 = torch.tensor(xavier_init([h_dim1,h_dim2]),requires_grad = True)\nG_b2 = torch.tensor(np.zeros(h_dim2), requires_grad = True)\n\nG_w3 = torch.tensor(xavier_init([h_dim2,dim]),requires_grad = True)\nG_b3 = torch.tensor(np.zeros(dim), requires_grad = True)\n\ndelta_G = [G_w1,G_w2,G_w3,G_b1,G_b2,G_b3]\n\n# Discrimnator architecture\nD_w1 = torch.tensor(xavier_init([dim*2,h_dim1]),requires_grad = True)      # Data + Hint\nD_b1 = torch.tensor(np.zeros(h_dim1), requires_grad = True)\n\nD_w2 = torch.tensor(xavier_init([h_dim1, h_dim2]),requires_grad = True)\nD_b2 = torch.tensor(np.zeros(h_dim2), requires_grad = True)\n\nD_w3 = torch.tensor(xavier_init([h_dim2,dim]),requires_grad = True)\nD_b3 = torch.tensor(np.zeros(dim), requires_grad = True)\n\ndelta_D = [D_w1,D_w2,D_w3,D_b1,D_b2,D_b3]","metadata":{"execution":{"iopub.status.busy":"2022-07-02T18:46:02.084088Z","iopub.execute_input":"2022-07-02T18:46:02.084475Z","iopub.status.idle":"2022-07-02T18:46:02.101196Z","shell.execute_reply.started":"2022-07-02T18:46:02.084428Z","shell.execute_reply":"2022-07-02T18:46:02.100056Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"def generator(new_x,mask):\n    inp = torch.cat((new_x,mask),1)\n    G_h1 = func.relu(torch.matmul(inp,G_w1) + G_b1)\n    G_h2 = func.relu(torch.matmul(G_h1,G_w2) + G_b2)\n    G_output = torch.sigmoid(torch.matmul(G_h2,G_w3)+G_b3)\n    return G_output \n\ndef discrimnator(new_x,hint):\n    inp = torch.cat((new_x,hint),1)\n    D_h1 = func.relu(torch.matmul(inp,D_w1)+D_b1)\n    D_h2 = func.relu(torch.matmul(D_h1,D_w2)+D_b2)\n    D_output = torch.sigmoid(torch.matmul(D_h2,D_w3)+D_b3)\n    return D_output\n","metadata":{"execution":{"iopub.status.busy":"2022-07-02T18:46:06.786438Z","iopub.execute_input":"2022-07-02T18:46:06.786809Z","iopub.status.idle":"2022-07-02T18:46:06.795376Z","shell.execute_reply.started":"2022-07-02T18:46:06.786781Z","shell.execute_reply":"2022-07-02T18:46:06.794493Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"def discrimnator_loss(mask,hint,new_x):\n    gen = generator(new_x,mask)\n    h_new_x = mask*new_x + (1-mask)*gen\n    disc = discrimnator(h_new_x,hint)\n    LD = -torch.mean(mask*torch.log(disc + 1e-8) + (1-mask)*torch.log(1-disc + 1e-8))\n    return LD \n\ndef generator_loss(x,new_x,mask,hint):\n    gen = generator(new_x,mask)\n    h_new_x = mask*new_x + (1-mask)*gen\n    \n    disc = discrimnator(h_new_x,hint)\n    LG = -torch.mean((1-mask)*torch.log(disc + 1e-8))\n    \n    Lm_ini = mask*new_x\n    Lm_out = mask*gen\n    \n    LM = torch.mean((Lm_ini - Lm_out)**2)/torch.mean(mask)\n    \n    gen_loss = LG + alpha*LM\n    mse_test_loss = torch.mean(((1-mask)*x - (1-mask)*gen)**2)/torch.mean(1-mask)\n    \n    return gen_loss, LM, mse_test_loss","metadata":{"execution":{"iopub.status.busy":"2022-07-02T18:46:10.493423Z","iopub.execute_input":"2022-07-02T18:46:10.493768Z","iopub.status.idle":"2022-07-02T18:46:10.502565Z","shell.execute_reply.started":"2022-07-02T18:46:10.493744Z","shell.execute_reply":"2022-07-02T18:46:10.501492Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"def test_loss(x,new_x,m):\n    gen = generator(new_x,m)\n    mse_test_loss = torch.mean(((1-m)*x - (1-m)*gen)**2)/torch.mean(1-m)\n    \n    return gen, mse_test_loss   ","metadata":{"execution":{"iopub.status.busy":"2022-07-02T18:46:13.326424Z","iopub.execute_input":"2022-07-02T18:46:13.326759Z","iopub.status.idle":"2022-07-02T18:46:13.332711Z","shell.execute_reply.started":"2022-07-02T18:46:13.326735Z","shell.execute_reply":"2022-07-02T18:46:13.331384Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"optimD = torch.optim.Adam(params = delta_D)\noptimG = torch.optim.Adam(params = delta_G)","metadata":{"execution":{"iopub.status.busy":"2022-07-02T18:47:53.421026Z","iopub.execute_input":"2022-07-02T18:47:53.421388Z","iopub.status.idle":"2022-07-02T18:47:53.426701Z","shell.execute_reply.started":"2022-07-02T18:47:53.421358Z","shell.execute_reply":"2022-07-02T18:47:53.425473Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"for it in tqdm(range(50000)):\n    indX = sample_X(train_ind, min_batch)\n    X = trainX[indX,:]\n    Z = sample_Z(min_batch,dim)\n    M = MisstrainX[indX,:]\n    H = M * sample_M(min_batch,dim,1-miss_rate)\n    new_x = M * X + (1-M)*Z\n    \n    new_x = torch.tensor(new_x)\n    X = torch.tensor(X)\n    M = torch.tensor(M)\n    H = torch.tensor(H)\n    \n    optimD.zero_grad(set_to_none=False)\n    currD_loss = discrimnator_loss(M,H,new_x)\n    currD_loss.backward()\n    optimD.step()\n    \n    optimG.zero_grad(set_to_none=False)\n    currG_to_loss,mse_train_loss,mse_test_loss = generator_loss(X,new_x,M,H)\n    currG_to_loss.backward()\n    optimG.step()\n    \n    if it%1000 == 0:\n        print(\"For Iter: {n:}\".format(n=it),end = '/t')\n        print(\"Train loss : {test:.4} and Test loss: {train:.4}\".format(test=np.sqrt(mse_train_loss.item()),train=np.sqrt(mse_test_loss.item())))\n        ","metadata":{"execution":{"iopub.status.busy":"2022-07-02T19:01:55.878852Z","iopub.execute_input":"2022-07-02T19:01:55.879255Z","iopub.status.idle":"2022-07-02T19:38:52.810187Z","shell.execute_reply.started":"2022-07-02T19:01:55.879224Z","shell.execute_reply":"2022-07-02T19:38:52.809179Z"},"trusted":true},"execution_count":45,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/50000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"40a09652acb749239353a286a69a149b"}},"metadata":{}},{"name":"stdout","text":"For Iter: 0/tTrain loss : 0.3162 and Test loss: 0.3322\nFor Iter: 1000/tTrain loss : 0.09408 and Test loss: 0.1086\nFor Iter: 2000/tTrain loss : 0.08661 and Test loss: 0.114\nFor Iter: 3000/tTrain loss : 0.0795 and Test loss: 0.1177\nFor Iter: 4000/tTrain loss : 0.07769 and Test loss: 0.1093\nFor Iter: 5000/tTrain loss : 0.07631 and Test loss: 0.109\nFor Iter: 6000/tTrain loss : 0.07678 and Test loss: 0.1168\nFor Iter: 7000/tTrain loss : 0.07594 and Test loss: 0.09787\nFor Iter: 8000/tTrain loss : 0.07658 and Test loss: 0.09332\nFor Iter: 9000/tTrain loss : 0.07501 and Test loss: 0.1133\nFor Iter: 10000/tTrain loss : 0.07435 and Test loss: 0.1019\nFor Iter: 11000/tTrain loss : 0.07466 and Test loss: 0.1028\nFor Iter: 12000/tTrain loss : 0.07527 and Test loss: 0.1026\nFor Iter: 13000/tTrain loss : 0.07562 and Test loss: 0.1071\nFor Iter: 14000/tTrain loss : 0.0744 and Test loss: 0.09849\nFor Iter: 15000/tTrain loss : 0.07479 and Test loss: 0.1067\nFor Iter: 16000/tTrain loss : 0.07551 and Test loss: 0.106\nFor Iter: 17000/tTrain loss : 0.0737 and Test loss: 0.1017\nFor Iter: 18000/tTrain loss : 0.07495 and Test loss: 0.0957\nFor Iter: 19000/tTrain loss : 0.07341 and Test loss: 0.09249\nFor Iter: 20000/tTrain loss : 0.075 and Test loss: 0.1077\nFor Iter: 21000/tTrain loss : 0.0745 and Test loss: 0.09506\nFor Iter: 22000/tTrain loss : 0.07677 and Test loss: 0.09662\nFor Iter: 23000/tTrain loss : 0.0735 and Test loss: 0.1002\nFor Iter: 24000/tTrain loss : 0.07382 and Test loss: 0.1009\nFor Iter: 25000/tTrain loss : 0.07669 and Test loss: 0.09264\nFor Iter: 26000/tTrain loss : 0.07557 and Test loss: 0.09506\nFor Iter: 27000/tTrain loss : 0.07366 and Test loss: 0.1023\nFor Iter: 28000/tTrain loss : 0.07317 and Test loss: 0.1008\nFor Iter: 29000/tTrain loss : 0.07344 and Test loss: 0.09967\nFor Iter: 30000/tTrain loss : 0.07625 and Test loss: 0.104\nFor Iter: 31000/tTrain loss : 0.07431 and Test loss: 0.1056\nFor Iter: 32000/tTrain loss : 0.07291 and Test loss: 0.09541\nFor Iter: 33000/tTrain loss : 0.07462 and Test loss: 0.09632\nFor Iter: 34000/tTrain loss : 0.0739 and Test loss: 0.09998\nFor Iter: 35000/tTrain loss : 0.07419 and Test loss: 0.09803\nFor Iter: 36000/tTrain loss : 0.07506 and Test loss: 0.1024\nFor Iter: 37000/tTrain loss : 0.074 and Test loss: 0.0953\nFor Iter: 38000/tTrain loss : 0.07542 and Test loss: 0.09899\nFor Iter: 39000/tTrain loss : 0.07312 and Test loss: 0.1093\nFor Iter: 40000/tTrain loss : 0.0746 and Test loss: 0.1029\nFor Iter: 41000/tTrain loss : 0.07521 and Test loss: 0.09993\nFor Iter: 42000/tTrain loss : 0.0746 and Test loss: 0.1024\nFor Iter: 43000/tTrain loss : 0.0757 and Test loss: 0.1066\nFor Iter: 44000/tTrain loss : 0.07304 and Test loss: 0.09256\nFor Iter: 45000/tTrain loss : 0.07619 and Test loss: 0.1014\nFor Iter: 46000/tTrain loss : 0.07366 and Test loss: 0.1062\nFor Iter: 47000/tTrain loss : 0.07496 and Test loss: 0.1001\nFor Iter: 48000/tTrain loss : 0.07386 and Test loss: 0.1112\nFor Iter: 49000/tTrain loss : 0.07381 and Test loss: 0.09079\n","output_type":"stream"}]},{"cell_type":"code","source":"test_x = testX\ntest_no = num - train_ind\nZ = sample_Z(test_no,dim)\nM = missing_mat[rand_index[train_ind:], :]\nnew_test_x = M*test_x + (1-M)*Z\n\nM = torch.tensor(M)\nnew_test_x = torch.tensor(new_test_x)\n\ngen, mse_test_loss = test_loss(test_x,new_test_x,M)\n\nprint(\"Test Loss: {val:.4}\".format(val=np.sqrt(mse_test_loss.item())))\n","metadata":{"execution":{"iopub.status.busy":"2022-07-02T19:38:58.987097Z","iopub.execute_input":"2022-07-02T19:38:58.987491Z","iopub.status.idle":"2022-07-02T19:39:00.818160Z","shell.execute_reply.started":"2022-07-02T19:38:58.987455Z","shell.execute_reply":"2022-07-02T19:39:00.817398Z"},"trusted":true},"execution_count":46,"outputs":[{"name":"stdout","text":"Test Loss: 0.1025\n","output_type":"stream"}]},{"cell_type":"code","source":"imputed_data = (M * test_x + (1-M) * gen).detach().numpy()\n\nX_train = torch.tensor(trainX)\nM_train = torch.tensor(MisstrainX)\nsample = currG_to_loss\n\nimputed_data_train = (M_train * X_train + (1-M_train) * sample).detach().numpy()","metadata":{"execution":{"iopub.status.busy":"2022-07-02T19:39:07.772980Z","iopub.execute_input":"2022-07-02T19:39:07.773462Z","iopub.status.idle":"2022-07-02T19:39:08.753400Z","shell.execute_reply.started":"2022-07-02T19:39:07.773413Z","shell.execute_reply":"2022-07-02T19:39:08.752259Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"final_set_train = np.zeros(trainX.shape)\nfor i in range(dim):\n    minv = min_val[i]\n    maxv = max_val[i]\n    final_set_train[:,i] = imputed_data_train[:,i]*maxv + minv\n    \n\nfinal_set_test = np.zeros(testX.shape)\nfor i in range(dim):\n    minv = min_val[i]\n    maxv = max_val[i]\n    final_set_test[:,i] = imputed_data[:,i]*maxv + minv","metadata":{"execution":{"iopub.status.busy":"2022-07-02T19:39:12.064282Z","iopub.execute_input":"2022-07-02T19:39:12.064654Z","iopub.status.idle":"2022-07-02T19:39:13.343023Z","shell.execute_reply.started":"2022-07-02T19:39:12.064625Z","shell.execute_reply":"2022-07-02T19:39:13.341801Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"result = df.to_numpy()\nresult[rand_index[:train_ind]]","metadata":{"execution":{"iopub.status.busy":"2022-07-02T19:39:17.055571Z","iopub.execute_input":"2022-07-02T19:39:17.055932Z","iopub.status.idle":"2022-07-02T19:39:17.988079Z","shell.execute_reply.started":"2022-07-02T19:39:17.055903Z","shell.execute_reply":"2022-07-02T19:39:17.986777Z"},"trusted":true},"execution_count":49,"outputs":[{"execution_count":49,"output_type":"execute_result","data":{"text/plain":"array([[-0.618753,  0.221454,  1.15471 , ...,  4.569999, -1.75299 ,\n         0.096553],\n       [-0.105178,  0.264646,  0.413251, ...,  1.670806, -1.026491,\n        -1.745219],\n       [ 1.059536,  0.575812,  1.075676, ...,  0.      ,  1.86339 ,\n         0.270463],\n       ...,\n       [ 1.128586, -2.067664, -0.016712, ...,  4.895038, -0.070414,\n         0.74611 ],\n       [-0.316467, -0.397591, -0.51135 , ...,  1.515648,  1.168376,\n         0.881015],\n       [ 0.753785, -0.646093, -0.345656, ...,  2.326378,  4.297027,\n         0.479601]])"},"metadata":{}}]},{"cell_type":"code","source":"final_set_train","metadata":{"execution":{"iopub.status.busy":"2022-07-02T19:39:21.007112Z","iopub.execute_input":"2022-07-02T19:39:21.007559Z","iopub.status.idle":"2022-07-02T19:39:21.015501Z","shell.execute_reply.started":"2022-07-02T19:39:21.007528Z","shell.execute_reply":"2022-07-02T19:39:21.013889Z"},"trusted":true},"execution_count":50,"outputs":[{"execution_count":50,"output_type":"execute_result","data":{"text/plain":"array([[0.41692525, 0.50969527, 0.60249393, ..., 0.70384236, 0.39484188,\n        0.80204381],\n       [0.46985125, 0.51408764, 0.52835314, ..., 0.5770524 , 0.42703995,\n        0.65550224],\n       [0.58987978, 0.54573136, 0.59459107, ..., 0.5039833 , 0.55511803,\n        0.81588105],\n       ...,\n       [0.59699566, 0.27690568, 0.48535981, ..., 0.71805724, 0.46941281,\n        0.85372616],\n       [0.44807706, 0.4467421 , 0.43589942, ..., 0.5702669 , 0.52431536,\n        0.86445994],\n       [0.55837089, 0.42147093, 0.45246768, ..., 0.60572243, 0.6629756 ,\n        0.83252123]])"},"metadata":{}}]},{"cell_type":"code","source":"result[rand_index[:train_ind]] = final_set_train\nresult[rand_index[:train_ind]]","metadata":{"execution":{"iopub.status.busy":"2022-07-02T19:39:28.368735Z","iopub.execute_input":"2022-07-02T19:39:28.369128Z","iopub.status.idle":"2022-07-02T19:39:29.489078Z","shell.execute_reply.started":"2022-07-02T19:39:28.369090Z","shell.execute_reply":"2022-07-02T19:39:29.487926Z"},"trusted":true},"execution_count":51,"outputs":[{"execution_count":51,"output_type":"execute_result","data":{"text/plain":"array([[0.41692525, 0.50969527, 0.60249393, ..., 0.70384236, 0.39484188,\n        0.80204381],\n       [0.46985125, 0.51408764, 0.52835314, ..., 0.5770524 , 0.42703995,\n        0.65550224],\n       [0.58987978, 0.54573136, 0.59459107, ..., 0.5039833 , 0.55511803,\n        0.81588105],\n       ...,\n       [0.59699566, 0.27690568, 0.48535981, ..., 0.71805724, 0.46941281,\n        0.85372616],\n       [0.44807706, 0.4467421 , 0.43589942, ..., 0.5702669 , 0.52431536,\n        0.86445994],\n       [0.55837089, 0.42147093, 0.45246768, ..., 0.60572243, 0.6629756 ,\n        0.83252123]])"},"metadata":{}}]},{"cell_type":"code","source":"final_set_test","metadata":{"execution":{"iopub.status.busy":"2022-07-02T19:39:32.629195Z","iopub.execute_input":"2022-07-02T19:39:32.629598Z","iopub.status.idle":"2022-07-02T19:39:32.638102Z","shell.execute_reply.started":"2022-07-02T19:39:32.629566Z","shell.execute_reply":"2022-07-02T19:39:32.636928Z"},"trusted":true},"execution_count":52,"outputs":[{"execution_count":52,"output_type":"execute_result","data":{"text/plain":"array([[0.44938605, 0.36476269, 0.59349655, ..., 0.62889244, 0.4272311 ,\n        0.70614244],\n       [0.4957607 , 0.6081064 , 0.48738067, ..., 0.63116279, 0.51879785,\n        0.74192775],\n       [0.30266632, 0.62778236, 0.53611001, ..., 0.62372987, 0.57894344,\n        0.79969392],\n       ...,\n       [0.52385921, 0.44516066, 0.42266034, ..., 0.51204904, 0.60660253,\n        0.74539999],\n       [0.54175507, 0.35085685, 0.52947117, ..., 0.66513809, 0.56487031,\n        0.80746524],\n       [0.36632993, 0.39450992, 0.42011731, ..., 0.55749389, 0.39936677,\n        0.80710091]])"},"metadata":{}}]},{"cell_type":"code","source":"result[rand_index[train_ind:]] = final_set_test\nresult[rand_index[train_ind:]]","metadata":{"execution":{"iopub.status.busy":"2022-07-02T19:39:37.042417Z","iopub.execute_input":"2022-07-02T19:39:37.042813Z","iopub.status.idle":"2022-07-02T19:39:37.317836Z","shell.execute_reply.started":"2022-07-02T19:39:37.042783Z","shell.execute_reply":"2022-07-02T19:39:37.316629Z"},"trusted":true},"execution_count":53,"outputs":[{"execution_count":53,"output_type":"execute_result","data":{"text/plain":"array([[0.44938605, 0.36476269, 0.59349655, ..., 0.62889244, 0.4272311 ,\n        0.70614244],\n       [0.4957607 , 0.6081064 , 0.48738067, ..., 0.63116279, 0.51879785,\n        0.74192775],\n       [0.30266632, 0.62778236, 0.53611001, ..., 0.62372987, 0.57894344,\n        0.79969392],\n       ...,\n       [0.52385921, 0.44516066, 0.42266034, ..., 0.51204904, 0.60660253,\n        0.74539999],\n       [0.54175507, 0.35085685, 0.52947117, ..., 0.66513809, 0.56487031,\n        0.80746524],\n       [0.36632993, 0.39450992, 0.42011731, ..., 0.55749389, 0.39936677,\n        0.80710091]])"},"metadata":{}}]},{"cell_type":"code","source":"result","metadata":{"execution":{"iopub.status.busy":"2022-07-02T19:39:41.271712Z","iopub.execute_input":"2022-07-02T19:39:41.272068Z","iopub.status.idle":"2022-07-02T19:39:41.280322Z","shell.execute_reply.started":"2022-07-02T19:39:41.272039Z","shell.execute_reply":"2022-07-02T19:39:41.279502Z"},"trusted":true},"execution_count":54,"outputs":[{"execution_count":54,"output_type":"execute_result","data":{"text/plain":"array([[0.44414823, 0.43998484, 0.7174265 , ..., 0.6705838 , 0.52733396,\n        0.88841235],\n       [0.62300177, 0.43636575, 0.44517898, ..., 0.31950683, 0.35176807,\n        0.78928722],\n       [0.50707449, 0.37939188, 0.48703089, ..., 0.41075594, 0.63480932,\n        0.7838926 ],\n       ...,\n       [0.49589425, 0.47559618, 0.4405292 , ..., 0.48784436, 0.40640457,\n        0.71499474],\n       [0.30447955, 0.40441766, 0.30039017, ..., 0.45650894, 0.61091774,\n        0.7947459 ],\n       [0.39758953, 0.48460685, 0.39948924, ..., 0.60606149, 0.71298546,\n        0.72841386]])"},"metadata":{}}]},{"cell_type":"code","source":"result = result.reshape(80,1000000)","metadata":{"execution":{"iopub.status.busy":"2022-07-02T19:40:34.483795Z","iopub.execute_input":"2022-07-02T19:40:34.484201Z","iopub.status.idle":"2022-07-02T19:40:34.489356Z","shell.execute_reply.started":"2022-07-02T19:40:34.484169Z","shell.execute_reply":"2022-07-02T19:40:34.488512Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"ans = pd.DataFrame(result,index=list(df.columns))\nans.head(6)","metadata":{"execution":{"iopub.status.busy":"2022-07-02T19:41:22.830888Z","iopub.execute_input":"2022-07-02T19:41:22.831235Z","iopub.status.idle":"2022-07-02T19:41:22.895938Z","shell.execute_reply.started":"2022-07-02T19:41:22.831209Z","shell.execute_reply":"2022-07-02T19:41:22.894865Z"},"trusted":true},"execution_count":57,"outputs":[{"execution_count":57,"output_type":"execute_result","data":{"text/plain":"         0         1         2         3         4         5         6       \\\nF_1_0  0.444148  0.439985  0.717426  0.550378  0.690761  0.539166  0.460840   \nF_1_1  0.487940  0.387962  0.342734  0.433148  0.567018  0.573500  0.588145   \nF_1_2  0.435798  0.293478  0.617359  0.340341  0.529532  0.302307  0.476647   \nF_1_3  0.446829  0.487175  0.476223  0.480530  0.415416  0.494383  0.623866   \nF_1_4  0.475259  0.294165  0.413495  0.545859  0.528448  0.515489  0.369063   \nF_1_5  0.537493  0.547273  0.433632  0.461257  0.472466  0.516880  0.628524   \n\n         7         8         9       ...    999990    999991    999992  \\\nF_1_0  0.787248  0.287124  0.459515  ...  0.452837  0.497565  0.596678   \nF_1_1  0.770924  0.480413  0.460437  ...  0.507039  0.625399  0.493115   \nF_1_2  0.757984  0.516173  0.569385  ...  0.419238  0.368480  0.477585   \nF_1_3  0.636480  0.389068  0.588349  ...  0.546074  0.592219  0.665611   \nF_1_4  0.564205  0.516917  0.413280  ...  0.525413  0.379308  0.559791   \nF_1_5  0.688602  0.505167  0.488864  ...  0.616965  0.519109  0.570031   \n\n         999993    999994    999995    999996    999997    999998    999999  \nF_1_0  0.802597  0.055116  0.789514  0.493720  0.454989  0.435071  0.823387  \nF_1_1  0.769459  0.781098  0.777231  0.480120  0.489384  0.418727  0.795440  \nF_1_2  0.834987  0.764171  0.767288  0.515502  0.607371  0.472534  0.916405  \nF_1_3  0.803964  0.789341  0.809394  0.458642  0.568656  0.420154  0.748403  \nF_1_4  0.755506  0.795274  0.757794  0.419973  0.646004  0.626972  0.795972  \nF_1_5  0.857274  0.739132  0.823362  0.404749  0.547090  0.351243  0.645636  \n\n[6 rows x 1000000 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>999990</th>\n      <th>999991</th>\n      <th>999992</th>\n      <th>999993</th>\n      <th>999994</th>\n      <th>999995</th>\n      <th>999996</th>\n      <th>999997</th>\n      <th>999998</th>\n      <th>999999</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>F_1_0</th>\n      <td>0.444148</td>\n      <td>0.439985</td>\n      <td>0.717426</td>\n      <td>0.550378</td>\n      <td>0.690761</td>\n      <td>0.539166</td>\n      <td>0.460840</td>\n      <td>0.787248</td>\n      <td>0.287124</td>\n      <td>0.459515</td>\n      <td>...</td>\n      <td>0.452837</td>\n      <td>0.497565</td>\n      <td>0.596678</td>\n      <td>0.802597</td>\n      <td>0.055116</td>\n      <td>0.789514</td>\n      <td>0.493720</td>\n      <td>0.454989</td>\n      <td>0.435071</td>\n      <td>0.823387</td>\n    </tr>\n    <tr>\n      <th>F_1_1</th>\n      <td>0.487940</td>\n      <td>0.387962</td>\n      <td>0.342734</td>\n      <td>0.433148</td>\n      <td>0.567018</td>\n      <td>0.573500</td>\n      <td>0.588145</td>\n      <td>0.770924</td>\n      <td>0.480413</td>\n      <td>0.460437</td>\n      <td>...</td>\n      <td>0.507039</td>\n      <td>0.625399</td>\n      <td>0.493115</td>\n      <td>0.769459</td>\n      <td>0.781098</td>\n      <td>0.777231</td>\n      <td>0.480120</td>\n      <td>0.489384</td>\n      <td>0.418727</td>\n      <td>0.795440</td>\n    </tr>\n    <tr>\n      <th>F_1_2</th>\n      <td>0.435798</td>\n      <td>0.293478</td>\n      <td>0.617359</td>\n      <td>0.340341</td>\n      <td>0.529532</td>\n      <td>0.302307</td>\n      <td>0.476647</td>\n      <td>0.757984</td>\n      <td>0.516173</td>\n      <td>0.569385</td>\n      <td>...</td>\n      <td>0.419238</td>\n      <td>0.368480</td>\n      <td>0.477585</td>\n      <td>0.834987</td>\n      <td>0.764171</td>\n      <td>0.767288</td>\n      <td>0.515502</td>\n      <td>0.607371</td>\n      <td>0.472534</td>\n      <td>0.916405</td>\n    </tr>\n    <tr>\n      <th>F_1_3</th>\n      <td>0.446829</td>\n      <td>0.487175</td>\n      <td>0.476223</td>\n      <td>0.480530</td>\n      <td>0.415416</td>\n      <td>0.494383</td>\n      <td>0.623866</td>\n      <td>0.636480</td>\n      <td>0.389068</td>\n      <td>0.588349</td>\n      <td>...</td>\n      <td>0.546074</td>\n      <td>0.592219</td>\n      <td>0.665611</td>\n      <td>0.803964</td>\n      <td>0.789341</td>\n      <td>0.809394</td>\n      <td>0.458642</td>\n      <td>0.568656</td>\n      <td>0.420154</td>\n      <td>0.748403</td>\n    </tr>\n    <tr>\n      <th>F_1_4</th>\n      <td>0.475259</td>\n      <td>0.294165</td>\n      <td>0.413495</td>\n      <td>0.545859</td>\n      <td>0.528448</td>\n      <td>0.515489</td>\n      <td>0.369063</td>\n      <td>0.564205</td>\n      <td>0.516917</td>\n      <td>0.413280</td>\n      <td>...</td>\n      <td>0.525413</td>\n      <td>0.379308</td>\n      <td>0.559791</td>\n      <td>0.755506</td>\n      <td>0.795274</td>\n      <td>0.757794</td>\n      <td>0.419973</td>\n      <td>0.646004</td>\n      <td>0.626972</td>\n      <td>0.795972</td>\n    </tr>\n    <tr>\n      <th>F_1_5</th>\n      <td>0.537493</td>\n      <td>0.547273</td>\n      <td>0.433632</td>\n      <td>0.461257</td>\n      <td>0.472466</td>\n      <td>0.516880</td>\n      <td>0.628524</td>\n      <td>0.688602</td>\n      <td>0.505167</td>\n      <td>0.488864</td>\n      <td>...</td>\n      <td>0.616965</td>\n      <td>0.519109</td>\n      <td>0.570031</td>\n      <td>0.857274</td>\n      <td>0.739132</td>\n      <td>0.823362</td>\n      <td>0.404749</td>\n      <td>0.547090</td>\n      <td>0.351243</td>\n      <td>0.645636</td>\n    </tr>\n  </tbody>\n</table>\n<p>6 rows Ã— 1000000 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"for i,col,val in zip(range(res.shape[0]),res['row-col'],res['value']):\n    x = col.split(\"-\")\n    val = ans.loc[x[1],int(x[0])]\n    res['value'][i] = val","metadata":{"execution":{"iopub.status.busy":"2022-07-02T19:42:33.761897Z","iopub.execute_input":"2022-07-02T19:42:33.762643Z","iopub.status.idle":"2022-07-02T19:47:54.190918Z","shell.execute_reply.started":"2022-07-02T19:42:33.762565Z","shell.execute_reply":"2022-07-02T19:47:54.189882Z"},"trusted":true},"execution_count":58,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  after removing the cwd from sys.path.\n","output_type":"stream"}]},{"cell_type":"code","source":"res","metadata":{"execution":{"iopub.status.busy":"2022-07-02T19:55:20.545861Z","iopub.execute_input":"2022-07-02T19:55:20.546254Z","iopub.status.idle":"2022-07-02T19:55:20.561835Z","shell.execute_reply.started":"2022-07-02T19:55:20.546217Z","shell.execute_reply":"2022-07-02T19:55:20.560765Z"},"trusted":true},"execution_count":59,"outputs":[{"execution_count":59,"output_type":"execute_result","data":{"text/plain":"              row-col     value\n0            0-F_1_14  0.266342\n1            0-F_3_23  0.594269\n2            1-F_3_24  0.603969\n3             2-F_1_2  0.617359\n4             2-F_4_2  0.300113\n...               ...       ...\n999995   999993-F_4_2  0.887132\n999996  999994-F_3_10  0.674869\n999997   999994-F_4_9  0.778133\n999998  999997-F_3_14  0.313218\n999999   999997-F_4_8  0.416937\n\n[1000000 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>row-col</th>\n      <th>value</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0-F_1_14</td>\n      <td>0.266342</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0-F_3_23</td>\n      <td>0.594269</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1-F_3_24</td>\n      <td>0.603969</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2-F_1_2</td>\n      <td>0.617359</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2-F_4_2</td>\n      <td>0.300113</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>999995</th>\n      <td>999993-F_4_2</td>\n      <td>0.887132</td>\n    </tr>\n    <tr>\n      <th>999996</th>\n      <td>999994-F_3_10</td>\n      <td>0.674869</td>\n    </tr>\n    <tr>\n      <th>999997</th>\n      <td>999994-F_4_9</td>\n      <td>0.778133</td>\n    </tr>\n    <tr>\n      <th>999998</th>\n      <td>999997-F_3_14</td>\n      <td>0.313218</td>\n    </tr>\n    <tr>\n      <th>999999</th>\n      <td>999997-F_4_8</td>\n      <td>0.416937</td>\n    </tr>\n  </tbody>\n</table>\n<p>1000000 rows Ã— 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"res.to_csv('Output_Playgroundjune2022',index= False)","metadata":{"execution":{"iopub.status.busy":"2022-07-02T19:55:25.060268Z","iopub.execute_input":"2022-07-02T19:55:25.060629Z","iopub.status.idle":"2022-07-02T19:55:27.004811Z","shell.execute_reply.started":"2022-07-02T19:55:25.060600Z","shell.execute_reply":"2022-07-02T19:55:27.003877Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}